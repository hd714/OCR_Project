"""
Evaluation Framework for Multimodal Document Processing
Tests and compares different OCR and embedding strategies
"""

import sys
from pathlib import Path
import json
import time
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
import pandas as pd
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.progress import track

# Import our components
sys.path.insert(0, str(Path(__file__).parent / "Biotech_Model_Test"))
sys.path.insert(0, str(Path(__file__).parent / "Biotech_Model_Test" / "src"))

from base_ocr import OCRResult, OCRBenchmarker
from main import OCRPipeline
from parse_pdfplumber import PDFPlumberParser
from parse_pypdf import PyPDFParser
from milvus_integration import MilvusManager

console = Console()

class EvaluationFramework:
    """Comprehensive evaluation of multimodal document processing approaches"""
    
    def __init__(self,
                 test_documents_dir: Path = None,
                 output_dir: Path = None):
        """
        Initialize evaluation framework
        
        Args:
            test_documents_dir: Directory containing test documents
            output_dir: Directory for saving results
        """
        self.test_documents_dir = test_documents_dir or Path("test_documents")
        self.output_dir = output_dir or Path("evaluation_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'approaches': {},
            'metrics': {},
            'best_configuration': None
        }
        
        # Initialize components
        self.ocr_pipeline = None
        self.milvus_manager = None
        self.embedding_models = {}
        
        # Test queries for evaluation
        self.test_queries = self._create_test_queries()
        
    def _create_test_queries(self) -> List[Dict[str, Any]]:
        """Create test queries for evaluation"""
        return [
            {
                'query': 'HUMIRA efficacy percentage',
                'expected_content': ['75%', 'efficacy', 'HUMIRA', 'Phase 3'],
                'query_type': 'drug_efficacy'
            },
            {
                'query': 'clinical trial phase results',
                'expected_content': ['Phase', 'results', 'trial', 'patients'],
                'query_type': 'clinical_data'
            },
            {
                'query': 'adverse events table',
                'expected_content': ['adverse', 'events', 'percentage', 'patients'],
                'query_type': 'table_data'
            },
            {
                'query': 'drug dosage recommendations',
                'expected_content': ['mg', 'daily', 'dose', 'administration'],
                'query_type': 'dosage_info'
            },
            {
                'query': 'statistical significance p-value',
                'expected_content': ['p-value', '<0.05', 'significant', 'statistical'],
                'query_type': 'statistics'
            }
        ]
    
    def evaluate_approach_1_ocr_to_text(self) -> Dict[str, Any]:
        """
        Approach 1: OCR â†’ Text â†’ Embed
        Extract everything to text first, then create text embeddings
        """
        console.print("\n[bold cyan]Evaluating Approach 1: OCR â†’ Text â†’ Embed[/bold cyan]")
        
        approach_results = {
            'method': 'ocr_to_text_embed',
            'extraction_results': [],
            'search_quality': {},
            'processing_times': {}
        }
        
        # Test different OCR models
        ocr_models = ['tesseract', 'pdfplumber']  # Add more as needed
        
        for model_name in ocr_models:
            console.print(f"\n[yellow]Testing {model_name}...[/yellow]")
            
            start_time = time.time()
            
            # Process documents
            if model_name == 'pdfplumber':
                parser = PDFPlumberParser()
            elif model_name == 'pypdf':
                parser = PyPDFParser()
            else:
                # Use OCR pipeline for image-based extraction
                self.ocr_pipeline = OCRPipeline(
                    models=[model_name],
                    enable_gpu=False,
                    save_full_text=True
                )
            
            extraction_quality = self._test_extraction_quality(model_name)
            
            # Test semantic search
            search_quality = self._test_semantic_search(
                model_name,
                approach='text_only'
            )
            
            processing_time = time.time() - start_time
            
            approach_results['extraction_results'].append({
                'model': model_name,
                'extraction_quality': extraction_quality,
                'processing_time': processing_time
            })
            
            approach_results['search_quality'][model_name] = search_quality
            approach_results['processing_times'][model_name] = processing_time
        
        return approach_results
    
    def evaluate_approach_2_multimodal(self) -> Dict[str, Any]:
        """
        Approach 2: Separate Text + Image Embeddings
        Extract text and images separately, create dual embeddings
        """
        console.print("\n[bold cyan]Evaluating Approach 2: Multimodal (Text + Image)[/bold cyan]")
        
        approach_results = {
            'method': 'multimodal_separate',
            'embedding_methods': [],
            'search_quality': {},
            'fusion_strategies': {}
        }
        
        # Test different embedding combinations
        embedding_strategies = [
            {
                'name': 'separate_indices',
                'text_model': 'sentence-transformers',
                'image_model': 'clip',
                'fusion': 'none'
            },
            {
                'name': 'concatenated',
                'text_model': 'sentence-transformers',
                'image_model': 'clip',
                'fusion': 'concatenate'
            },
            {
                'name': 'weighted_average',
                'text_model': 'sentence-transformers',
                'image_model': 'clip',
                'fusion': 'weighted',
                'weights': {'text': 0.7, 'image': 0.3}
            }
        ]
        
        for strategy in embedding_strategies:
            console.print(f"\n[yellow]Testing {strategy['name']}...[/yellow]")
            
            start_time = time.time()
            
            # Test multimodal search
            search_quality = self._test_multimodal_search(strategy)
            
            processing_time = time.time() - start_time
            
            approach_results['embedding_methods'].append(strategy)
            approach_results['search_quality'][strategy['name']] = search_quality
            approach_results['fusion_strategies'][strategy['name']] = {
                'quality': search_quality,
                'time': processing_time
            }
        
        return approach_results
    
    def _test_extraction_quality(self, model_name: str) -> Dict[str, float]:
        """Test extraction quality for specific content types"""
        quality_metrics = {
            'drug_names_found': 0,
            'percentages_found': 0,
            'tables_preserved': 0,
            'text_completeness': 0,
            'overall_score': 0
        }
        
        # Create test document with known content
        test_content = self._create_test_document()
        
        # Process with model
        # (Implementation depends on specific model)
        
        # Check for specific content
        # This is simplified - you'd implement actual checking
        quality_metrics['drug_names_found'] = 0.85  # Mock score
        quality_metrics['percentages_found'] = 0.90
        quality_metrics['tables_preserved'] = 0.75
        quality_metrics['text_completeness'] = 0.88
        
        quality_metrics['overall_score'] = np.mean([
            quality_metrics['drug_names_found'],
            quality_metrics['percentages_found'],
            quality_metrics['tables_preserved'],
            quality_metrics['text_completeness']
        ])
        
        return quality_metrics
    
    def _test_semantic_search(self, 
                             model_name: str,
                             approach: str) -> Dict[str, float]:
        """Test semantic search quality"""
        search_metrics = {
            'precision_at_5': 0,
            'recall_at_5': 0,
            'mrr': 0,  # Mean Reciprocal Rank
            'specific_info_retrieval': {}
        }
        
        # Test each query
        for query in self.test_queries:
            # Mock search (replace with actual implementation)
            results = self._mock_search(query['query'], approach)
            
            # Check if expected content is found
            found_content = self._check_content_in_results(
                results,
                query['expected_content']
            )
            
            search_metrics['specific_info_retrieval'][query['query_type']] = {
                'found': found_content,
                'score': len(found_content) / len(query['expected_content'])
            }
        
        # Calculate overall metrics
        search_metrics['precision_at_5'] = 0.82  # Mock
        search_metrics['recall_at_5'] = 0.78  # Mock
        search_metrics['mrr'] = 0.85  # Mock
        
        return search_metrics
    
    def _test_multimodal_search(self, strategy: Dict[str, Any]) -> Dict[str, float]:
        """Test multimodal search with different strategies"""
        return {
            'text_search_quality': 0.85,
            'image_search_quality': 0.75,
            'combined_search_quality': 0.88,
            'table_retrieval_accuracy': 0.82,
            'figure_retrieval_accuracy': 0.79
        }
    
    def _create_test_document(self) -> Dict[str, Any]:
        """Create test document with known content for evaluation"""
        return {
            'content': """
            Clinical Trial Results for HUMIRA
            
            Phase 3 Trial Summary:
            - Efficacy: 75% response rate
            - Patients: n=500
            - Duration: 52 weeks
            - p-value: <0.001
            
            Table 1: Adverse Events
            Event Type | Percentage | Severity
            -----------|------------|----------
            Headache   | 15%        | Mild
            Nausea     | 8%         | Mild
            Fatigue    | 12%        | Moderate
            
            Dosage: 40mg subcutaneous every 2 weeks
            """,
            'expected_extracts': {
                'drug_name': 'HUMIRA',
                'efficacy': '75%',
                'patient_count': '500',
                'p_value': '<0.001',
                'dosage': '40mg',
                'has_table': True
            }
        }
    
    def _mock_search(self, query: str, approach: str) -> List[Dict[str, Any]]:
        """Mock search results for testing"""
        return [
            {
                'text': 'HUMIRA showed 75% efficacy...',
                'score': 0.95,
                'metadata': {'page': 1, 'has_table': True}
            }
        ]
    
    def _check_content_in_results(self,
                                  results: List[Dict[str, Any]],
                                  expected: List[str]) -> List[str]:
        """Check which expected content is found in results"""
        found = []
        combined_text = ' '.join([r.get('text', '') for r in results]).lower()
        
        for expected_item in expected:
            if expected_item.lower() in combined_text:
                found.append(expected_item)
        
        return found
    
    def generate_comparison_report(self) -> None:
        """Generate comprehensive comparison report"""
        console.print("\n[bold green]Generating Comparison Report[/bold green]")
        
        # Create comparison table
        table = Table(title="OCR & Embedding Approach Comparison")
        table.add_column("Approach", style="cyan")
        table.add_column("Extraction Quality", style="yellow")
        table.add_column("Search Precision", style="green")
        table.add_column("Search Recall", style="green")
        table.add_column("Processing Time", style="blue")
        table.add_column("Overall Score", style="magenta")
        
        # Add approach 1 results
        approach1_results = self.results['approaches'].get('approach_1', {})
        if approach1_results:
            for model in approach1_results.get('extraction_results', []):
                table.add_row(
                    f"OCRâ†’Text ({model['model']})",
                    f"{model.get('extraction_quality', {}).get('overall_score', 0):.2%}",
                    f"{approach1_results.get('search_quality', {}).get(model['model'], {}).get('precision_at_5', 0):.2%}",
                    f"{approach1_results.get('search_quality', {}).get(model['model'], {}).get('recall_at_5', 0):.2%}",
                    f"{model.get('processing_time', 0):.2f}s",
                    f"{self._calculate_overall_score(model, approach1_results):.2%}"
                )
        
        # Add approach 2 results
        approach2_results = self.results['approaches'].get('approach_2', {})
        if approach2_results:
            for method in approach2_results.get('embedding_methods', []):
                quality = approach2_results.get('search_quality', {}).get(method['name'], {})
                table.add_row(
                    f"Multimodal ({method['name']})",
                    "N/A",
                    f"{quality.get('combined_search_quality', 0):.2%}",
                    f"{quality.get('table_retrieval_accuracy', 0):.2%}",
                    f"{approach2_results.get('fusion_strategies', {}).get(method['name'], {}).get('time', 0):.2f}s",
                    f"{quality.get('combined_search_quality', 0):.2%}"
                )
        
        console.print(table)
        
        # Save detailed report
        report_path = self.output_dir / f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        console.print(f"\n[green]Report saved to: {report_path}[/green]")
        
        # Generate recommendations
        self._generate_recommendations()
    
    def _calculate_overall_score(self, model_data: Dict, approach_data: Dict) -> float:
        """Calculate overall score for an approach"""
        extraction_score = model_data.get('extraction_quality', {}).get('overall_score', 0)
        search_quality = approach_data.get('search_quality', {}).get(model_data['model'], {})
        search_score = np.mean([
            search_quality.get('precision_at_5', 0),
            search_quality.get('recall_at_5', 0)
        ])
        
        # Weight extraction and search equally
        return (extraction_score + search_score) / 2
    
    def _generate_recommendations(self) -> None:
        """Generate recommendations based on evaluation"""
        console.print("\n[bold yellow]RECOMMENDATIONS[/bold yellow]")
        
        recommendations = []
        
        # Analyze approach 1 results
        approach1 = self.results['approaches'].get('approach_1', {})
        if approach1:
            best_ocr = max(
                approach1.get('extraction_results', []),
                key=lambda x: x.get('extraction_quality', {}).get('overall_score', 0),
                default=None
            )
            if best_ocr:
                recommendations.append(
                    f"âœ… For text-heavy documents, use {best_ocr['model']} "
                    f"(extraction quality: {best_ocr.get('extraction_quality', {}).get('overall_score', 0):.2%})"
                )
        
        # Analyze approach 2 results
        approach2 = self.results['approaches'].get('approach_2', {})
        if approach2:
            best_multimodal = max(
                approach2.get('fusion_strategies', {}).items(),
                key=lambda x: x[1].get('quality', {}).get('combined_search_quality', 0),
                default=None
            )
            if best_multimodal:
                recommendations.append(
                    f"âœ… For documents with tables/figures, use {best_multimodal[0]} "
                    f"(search quality: {best_multimodal[1].get('quality', {}).get('combined_search_quality', 0):.2%})"
                )
        
        # Specific recommendations for biotech documents
        recommendations.extend([
            "ðŸ“Š For clinical trial documents with tables: Use PDFPlumber + Multimodal embeddings",
            "ðŸ’Š For drug efficacy data: Prioritize table preservation during extraction",
            "ðŸ”¬ For mixed content (text + figures): Use weighted fusion (70% text, 30% image)",
            "âš¡ For real-time processing: Use PyPDF2 for text extraction + cached embeddings"
        ])
        
        for rec in recommendations:
            console.print(f"  {rec}")
        
        self.results['recommendations'] = recommendations
    
    def run_full_evaluation(self) -> None:
        """Run complete evaluation pipeline"""
        console.print("[bold cyan]Starting Full Evaluation Pipeline[/bold cyan]")
        
        try:
            # Approach 1: OCR to Text
            self.results['approaches']['approach_1'] = self.evaluate_approach_1_ocr_to_text()
            
            # Approach 2: Multimodal
            self.results['approaches']['approach_2'] = self.evaluate_approach_2_multimodal()
            
            # Generate comparison report
            self.generate_comparison_report()
            
            # Determine best configuration
            self._determine_best_configuration()
            
            console.print("\n[bold green]âœ… Evaluation Complete![/bold green]")
            
        except Exception as e:
            console.print(f"[bold red]âŒ Evaluation failed: {e}[/bold red]")
            raise
    
    def _determine_best_configuration(self) -> None:
        """Determine the best configuration based on results"""
        best_config = {
            'for_text_documents': None,
            'for_mixed_content': None,
            'for_speed': None,
            'for_accuracy': None
        }
        
        # Analyze results to determine best configurations
        # (Implementation would analyze actual results)
        
        best_config['for_text_documents'] = {
            'method': 'PDFPlumber + Sentence Transformers',
            'reason': 'Best text extraction quality with native PDF parsing'
        }
        
        best_config['for_mixed_content'] = {
            'method': 'OCR + CLIP Multimodal',
            'reason': 'Preserves both textual and visual information'
        }
        
        best_config['for_speed'] = {
            'method': 'PyPDF2 + Cached Embeddings',
            'reason': 'Fastest processing with acceptable quality'
        }
        
        best_config['for_accuracy'] = {
            'method': 'Tesseract Advanced + Weighted Multimodal',
            'reason': 'Highest extraction and search quality'
        }
        
        self.results['best_configuration'] = best_config
        
        console.print("\n[bold green]Best Configurations:[/bold green]")
        for use_case, config in best_config.items():
            console.print(f"  {use_case}: {config['method']}")
            console.print(f"    Reason: {config['reason']}")


def main():
    """Run the evaluation"""
    evaluator = EvaluationFramework()
    evaluator.run_full_evaluation()


if __name__ == "__main__":
    main()